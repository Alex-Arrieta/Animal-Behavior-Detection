{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13747775,"sourceType":"datasetVersion","datasetId":8670263}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":1486.84424,"end_time":"2025-12-02T21:32:33.249407","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-02T21:07:46.405167","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ec5f8645","cell_type":"markdown","source":"Imports","metadata":{}},{"id":"dbbf8eb4","cell_type":"code","source":"#import os\n#os.environ[\"KERAS_BACKEND\"] = \"jax\"\nimport keras\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn import svm, naive_bayes, neighbors, model_selection, ensemble, decomposition, metrics, calibration, linear_model\nimport numpy as np\nimport tensorflow as tf\nimport keras_hub\nimport glob\nimport time\nimport json\nimport gc\nimport itertools\nfrom joblib import dump","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:48:48.273406Z","iopub.execute_input":"2026-02-06T15:48:48.274634Z","iopub.status.idle":"2026-02-06T15:49:12.509882Z","shell.execute_reply.started":"2026-02-06T15:48:48.274595Z","shell.execute_reply":"2026-02-06T15:49:12.508365Z"},"papermill":{"duration":16.438764,"end_time":"2025-12-02T21:08:06.284356","exception":false,"start_time":"2025-12-02T21:07:49.845592","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"2026-02-06 15:48:50.691329: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770392931.019944      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770392931.111896      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"id":"f60b66ef","cell_type":"markdown","source":"Data input and execution parameters","metadata":{}},{"id":"87b34582","cell_type":"code","source":"paths = [\"/kaggle/input/data-cleaned-10/data_cleaned/data_cleaned_10/AFS*.csv\", \"/kaggle/input/data-cleaned-10/data_cleaned/data_cleaned_10/NZFS*.csv\", \"/kaggle/input/data-cleaned-10/data_cleaned/data_cleaned_10/ASL*.csv\"]\nall_files = []\nfor path in paths:\n  all_files.extend(glob.glob(path))\nall_files\n\nfull_df = pd.DataFrame(columns=[])\n\nfor filename in all_files:\n    temp = pd.read_csv(filename)\n    temp[\"species\"] = filename.split(\"/\")[6].split(\"_\")[0]\n    full_df = pd.concat([full_df, temp], join = \"outer\")\ndf = full_df.reset_index(drop=True)\ndf[\"species\"] = df[\"species\"].map({\"AFS\" : 0, \"ASL\" : 1, \"BLKI\" : 2, \"cat\" : 3, \"dingo\" : 4, \"duck\" : 5, \"goat\" : 6, \"ibex\" : 7, \"NZFS\" : 8, \"sheep\" : 9, \"squirrel\" : 10, \"stork\" : 11, \"TBMU\" : 12})\n\nsample_name = \"temp\"\nsample_rate = 10 #Sample rate in Hz\ntime_window = 5 #Amount of seconds to look back\ntransfer = True #Should transfer learning be done\nuse_species = True #Will there be a species index included\nnormalized = True #Use normalized input variables (mean = 0, std = 1)\nmax_samples = 100000 #Max seconds of data to include from each species (Will be multplied by sample rate)\ndel full_df #Used to clean up ram\ndel temp #Used to clean up ram\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:12.513383Z","iopub.execute_input":"2026-02-06T15:49:12.514578Z","iopub.status.idle":"2026-02-06T15:49:15.183594Z","shell.execute_reply.started":"2026-02-06T15:49:12.514418Z","shell.execute_reply":"2026-02-06T15:49:15.181708Z"},"papermill":{"duration":2.271699,"end_time":"2025-12-02T21:08:08.577399","exception":false,"start_time":"2025-12-02T21:08:06.305700","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":2},{"id":"ac7cb0db","cell_type":"code","source":"for species_id in df[\"species\"].unique():\n    ratio = (max_samples * sample_rate) / len(df[df[\"species\"] == species_id].index)\n    if ratio < 1: #Undersamples over represented animals\n        for segment_id in df[df[\"species\"] == species_id][\"segment_id\"].unique():\n            to_del = df[df[\"segment_id\"] == segment_id]\n            to_del_index = to_del.tail(int((1-ratio) * len(to_del.index))).index\n            df.drop(to_del_index, axis=0, inplace=True)\ndf = df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.189871Z","iopub.execute_input":"2026-02-06T15:49:15.190225Z","iopub.status.idle":"2026-02-06T15:49:15.279154Z","shell.execute_reply.started":"2026-02-06T15:49:15.190198Z","shell.execute_reply":"2026-02-06T15:49:15.277894Z"},"papermill":{"duration":0.06366,"end_time":"2025-12-02T21:08:08.646590","exception":false,"start_time":"2025-12-02T21:08:08.582930","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":3},{"id":"f484c4fa","cell_type":"code","source":"animal_ids = df[\"species\"]\nsegment_ids = df[\"segment_id\"]\n\nX = df.drop([\"species\", \"segment_id\", \"label\", \"label_short\", \"label_long\", \"Unnamed: 0\"], axis = 1)\ny_short = df[\"label_short\"]\ny_long = df[\"label_long\"]\nY = y_short #Variable renaming for easy use throughout code\n\nif normalized:\n    X = (X - X.mean()) / X.std()\n\nif use_species:\n    X = X.join(animal_ids)","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.280614Z","iopub.execute_input":"2026-02-06T15:49:15.280977Z","iopub.status.idle":"2026-02-06T15:49:15.323737Z","shell.execute_reply.started":"2026-02-06T15:49:15.280943Z","shell.execute_reply":"2026-02-06T15:49:15.321764Z"},"papermill":{"duration":0.013041,"end_time":"2025-12-02T21:08:08.665007","exception":false,"start_time":"2025-12-02T21:08:08.651966","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"a0a87d41","cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.324748Z","iopub.execute_input":"2026-02-06T15:49:15.325109Z","iopub.status.idle":"2026-02-06T15:49:15.355615Z","shell.execute_reply.started":"2026-02-06T15:49:15.325059Z","shell.execute_reply":"2026-02-06T15:49:15.354012Z"},"papermill":{"duration":0.023436,"end_time":"2025-12-02T21:08:08.693353","exception":false,"start_time":"2025-12-02T21:08:08.669917","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"               x         y         z  species\n0       1.236916 -0.363895  0.291366        0\n1       1.348840 -0.571227  0.140651        0\n2       1.266762 -0.614259  0.111943        0\n3       1.266762 -0.610347  0.090413        0\n4       1.281685 -0.567315  0.068882        0\n...          ...       ...       ...      ...\n417788  0.598948  0.351989  1.321251        1\n417789 -0.501639 -0.168298  1.080825        1\n417790 -0.087520 -1.603978  0.564088        1\n417791  0.699680 -0.551668  1.066471        1\n417792  0.516871 -0.590787  0.155005        1\n\n[417793 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.236916</td>\n      <td>-0.363895</td>\n      <td>0.291366</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.348840</td>\n      <td>-0.571227</td>\n      <td>0.140651</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.266762</td>\n      <td>-0.614259</td>\n      <td>0.111943</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.266762</td>\n      <td>-0.610347</td>\n      <td>0.090413</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.281685</td>\n      <td>-0.567315</td>\n      <td>0.068882</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>417788</th>\n      <td>0.598948</td>\n      <td>0.351989</td>\n      <td>1.321251</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>417789</th>\n      <td>-0.501639</td>\n      <td>-0.168298</td>\n      <td>1.080825</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>417790</th>\n      <td>-0.087520</td>\n      <td>-1.603978</td>\n      <td>0.564088</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>417791</th>\n      <td>0.699680</td>\n      <td>-0.551668</td>\n      <td>1.066471</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>417792</th>\n      <td>0.516871</td>\n      <td>-0.590787</td>\n      <td>0.155005</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>417793 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"id":"d0fbf52b","cell_type":"code","source":"#Use this for ram cleanup as needed\ndel df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.358558Z","iopub.execute_input":"2026-02-06T15:49:15.358955Z","iopub.status.idle":"2026-02-06T15:49:15.677330Z","shell.execute_reply.started":"2026-02-06T15:49:15.358920Z","shell.execute_reply":"2026-02-06T15:49:15.676136Z"},"papermill":{"duration":0.251564,"end_time":"2025-12-02T21:08:08.949945","exception":false,"start_time":"2025-12-02T21:08:08.698381","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":6},{"id":"3fe4b275","cell_type":"markdown","source":"Training and Evalutation","metadata":{}},{"id":"28e9b645","cell_type":"code","source":"def compute_tss_keras(model, filtered_y, filtered_preds, behaviors, subtitle = \"\", epochs = 10, split = 0, species_name = \"NA\"):\n    #Following code used to display and save a formatted matplotlib confusion matrix\n    #fig, ax = plt.subplots(figsize=(8, 8), dpi = 160,)\n    #metrics.ConfusionMatrixDisplay.from_predictions(filtered_y, filtered_preds, ax = ax) #Fix sizing...\n    #fig.suptitle(f\"{model.name} on {species_name}\")\n    #plt.title(subtitle)\n    #fig.xticks(rotation=90)\n    #fig.tight_layout()\n    #fig.savefig(f\"Matrix_{model.name}_{subtitle}_{split}_{species_name}\")\n    #fig.show()\n\n    y_actu = pd.Categorical(filtered_y, categories = behaviors)\n    y_pred = pd.Categorical(filtered_preds, categories = behaviors)\n    df_conf = pd.crosstab(y_actu, y_pred, dropna=False)\n    df_conf.to_csv(f\"Matrix_{model.name}_{subtitle}_{split}_{species_name}.csv\")\n    to_save = {\"model\" : model.name, \"Train\" : subtitle, \"Samples\" : len(filtered_y), \"Accuracy\" : np.mean(filtered_y == filtered_preds), \"Epochs\" : epochs, \"species\" : species_name, \"TSS\" : {}, \"F1\" : {}, \"Precision\" : {}, \"Recall\" : {}}\n    for behavior in behaviors:\n        target = filtered_y == behavior\n        behavior_preds = filtered_preds == behavior\n        tempDF = pd.DataFrame(target, columns = [\"class.a\"])\n        tempDF[\"preds\"] = behavior_preds\n        tempDF[\"cat\"] = \"TN\"\n        tempDF.loc[(tempDF[\"class.a\"] == True) & (tempDF[\"preds\"] == True), \"cat\"] = \"TP\"\n        tempDF.loc[(tempDF[\"class.a\"] == False) & (tempDF[\"preds\"] == True), \"cat\"] = \"FP\"\n        tempDF.loc[(tempDF[\"class.a\"] == True) & (tempDF[\"preds\"] == False), \"cat\"] = \"FN\"\n        tp = (tempDF[\"cat\"] == \"TP\").sum()\n        tn = (tempDF[\"cat\"] == \"TN\").sum()\n        fn = (tempDF[\"cat\"] == \"FN\").sum()\n        fp = (tempDF[\"cat\"] == \"FP\").sum()\n        \n        if tp+fn == 0:\n            a = 0\n        else:\n            a = (tp/(tp+fn))\n        if tn+fp == 0:\n            b = 0\n        else:\n            b = (tn/(tn+fp))\n        \n        tss = a+b - 1\n        if tp+fp == 0:\n            precision = 0\n        else:\n            precision = tp/(tp+fp)\n        if tp+fn == 0:\n            recall = 0\n        else:\n            recall = tp/(tp+fn)\n        if tp+fp+fn == 0:\n            f1 = 0\n        else:\n            f1 = (2*tp)/((2*tp)+fp+fn)\n            \n        to_save[\"TSS\"][behavior] = tss\n        to_save[\"F1\"][behavior] = f1\n        to_save[\"Precision\"][behavior] = precision\n        to_save[\"Recall\"][behavior] = recall\n    with open(f\"{model.name}_{subtitle}_{split}_{species_name}.json\", 'w') as fp:\n        json.dump(to_save, fp)\n\n\ndef compute_tss(model, filtered_y, filtered_preds, behaviors, subtitle = \"\", split = 0, species_name = \"NA\"):\n    #Following code used to display and save a formatted matplotlib confusion matrix\n    #fig, ax = plt.subplots(figsize=(8, 8), dpi = 160,)\n    #metrics.ConfusionMatrixDisplay.from_predictions(filtered_y, filtered_preds, ax = ax) #Fix sizing...\n    #fig.suptitle(f\"{model.name} on {species_name}\")\n    #plt.title(subtitle)\n    #fig.xticks(rotation=90)\n    #fig.tight_layout()\n    #fig.savefig(f\"Matrix_{model.name}_{subtitle}_{split}_{species_name}\")\n    #fig.show()\n\n    y_actu = pd.Categorical(filtered_y, categories = behaviors)\n    y_pred = pd.Categorical(filtered_preds, categories = behaviors)\n    df_conf = pd.crosstab(y_actu, y_pred, dropna=False)\n    df_conf.to_csv(f\"Matrix_{type(model).__name__}_{subtitle}_{split}_{species_name}.csv\")\n    to_save = {\"model\" : type(model).__name__, \"Train\" : subtitle, \"Samples\" : len(filtered_y), \"Accuracy\" : np.mean(filtered_y == filtered_preds), \"species\" : species_name, \"TSS\" : {}, \"F1\" : {}, \"Precision\" : {}, \"Recall\" : {}}\n    for behavior in behaviors:\n        target = filtered_y == behavior\n        behavior_preds = filtered_preds == behavior\n        tempDF = pd.DataFrame(target, columns = [\"class.a\"])\n        tempDF[\"preds\"] = behavior_preds\n        tempDF[\"cat\"] = \"TN\"\n        tempDF.loc[(tempDF[\"class.a\"] == True) & (tempDF[\"preds\"] == True), \"cat\"] = \"TP\"\n        tempDF.loc[(tempDF[\"class.a\"] == False) & (tempDF[\"preds\"] == True), \"cat\"] = \"FP\"\n        tempDF.loc[(tempDF[\"class.a\"] == True) & (tempDF[\"preds\"] == False), \"cat\"] = \"FN\"\n        tp = (tempDF[\"cat\"] == \"TP\").sum()\n        tn = (tempDF[\"cat\"] == \"TN\").sum()\n        fn = (tempDF[\"cat\"] == \"FN\").sum()\n        fp = (tempDF[\"cat\"] == \"FP\").sum()\n        \n        if tp+fn == 0:\n            a = 0\n        else:\n            a = (tp/(tp+fn))\n        if tn+fp == 0:\n            b = 0\n        else:\n            b = (tn/(tn+fp))\n        \n        tss = a+b - 1\n        if tp+fp == 0:\n            precision = 0\n        else:\n            precision = tp/(tp+fp)\n        if tp+fn == 0:\n            recall = 0\n        else:\n            recall = tp/(tp+fn)\n        if tp+fp+fn == 0:\n            f1 = 0\n        else:\n            f1 = (2*tp)/((2*tp)+fp+fn)\n            \n        to_save[\"TSS\"][behavior] = tss\n        to_save[\"F1\"][behavior] = f1\n        to_save[\"Precision\"][behavior] = precision\n        to_save[\"Recall\"][behavior] = recall\n    with open(f\"{type(model).__name__}_{subtitle}_{split}_{species_name}.json\", 'w') as fp:\n        json.dump(to_save, fp)","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.678429Z","iopub.execute_input":"2026-02-06T15:49:15.678772Z","iopub.status.idle":"2026-02-06T15:49:15.702297Z","shell.execute_reply.started":"2026-02-06T15:49:15.678737Z","shell.execute_reply":"2026-02-06T15:49:15.701159Z"},"papermill":{"duration":0.022128,"end_time":"2025-12-02T21:08:08.978064","exception":false,"start_time":"2025-12-02T21:08:08.955936","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"id":"a4c6a69e","cell_type":"code","source":"def train_models_keras(model_architecutres, data_set, Y, num_features, splits = None, num_splits = 5, time_step = 1, batch_size = 64, epochs = 100, timed = False, threshold = 0, species = False, species_name = \"NA\", cache = True, val_freq = 100, transfer = False):\n    behaviors = sorted(Y.unique())\n    num_behaviors = len(behaviors)\n    for i in range(len(model_architecutres)):\n        cur_model = model_architecutres[i](num_features, time_step, len(behaviors))\n        input_size = cur_model.get_config()[\"layers\"][0][\"config\"][\"batch_shape\"]\n        if (len(input_size) == 2) and (input_size[1] != (num_features * time_step) and (not species)):\n            print(f\"The model expected inputs of {cur_model.name} does not match the given inputs\")\n            print(f\"Expected inputs: {input_size[1]}\")\n            print(f\"Given inputs: {num_features * time_step}\")\n            return\n        elif (len(input_size) == 2) and (input_size[1] != ((num_features-1) * time_step + 1)) and (species):\n            print(f\"The model expected inputs of {cur_model.name} does not match the given inputs\")\n            print(f\"Expected inputs: {input_size[1]}\")\n            print(f\"Given inputs: {((num_features-1) * time_step + 1)}\")\n            return\n        elif (len(input_size) == 3) and (input_size[1] != time_step) and (input_size[2] != num_features):\n            print(f\"The model expected inputs of {cur_model.name} does not match the given inputs\")\n            print(f\"Expected inputs: {input_size[1]}, {input_size[2]}\")\n            print(f\"Given inputs: {time_step}, {num_features}\")\n            return\n        elif (len(input_size) > 3) :\n            print(\"Too many channels requsted for model input\")\n            print(f\"Expected inputs: {input_size}\")\n            print(f\"Given inputs: {time_step}, {num_features}\")\n            return\n    \n    statistics = []\n    for i in range(len(model_architecutres)):\n        statistics.append([0, 0, pd.DataFrame(columns = Y.unique()), pd.DataFrame(columns = Y.unique())])\n    \n    if (splits == None) or (splits == []): #Expecting a zipped list of indicies i.e. ((train indices 1, test indicies 1), (train indices 2, test indicies 2), etc.)\n        #If user does not provide splits we take num_splits to make stratified splits via skf\n        skf = model_selection.StratifiedKFold(n_splits=num_splits)\n        splits = []\n        for train, test in skf.split(np.zeros(len(Y.index)), Y):\n            splits.append((-1, train, test))\n    else:\n        num_splits = len(splits)\n    \n    i = 0\n    for (animal_id, train_index, test_index) in splits:\n        keys_tensor = tf.constant(train_index, dtype = np.int64)\n        vals_tensor = tf.ones_like(keys_tensor)  # Ones will be casted to True.\n        \n        table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n        default_value=0)  # If index not in table, return 0.\n        \n        #This massive things just looks up the index in the table\n        #If the index is present, return 1, else return 0. It casts those to bools, the maps the output data to remove the index info. Then we rebatch.\n\n        if cache == True:\n            #If we cache the results then shuffling won't be a problem\n            filtered_ds_train = data_set.filter(lambda x, y: tf.cast(table.lookup(y[0]), tf.bool)).map(lambda a, b: (a, b[1:])).shuffle(100000).batch(batch_size)\n        else:\n            #If we don't cache shuffling causes the train and test sets to get mismatched\n            filtered_ds_train = data_set.filter(lambda x, y: tf.cast(table.lookup(y[0]), tf.bool)).map(lambda a, b: (a, b[1:])).batch(batch_size)\n        \n        #We do this over for the test indicies (could use ~tf.cast but this allows for the possibility of not using all the data on a split)\n        keys_tensor = tf.constant(test_index, dtype = np.int64)\n        vals_tensor = tf.ones_like(keys_tensor)\n        \n        table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n        default_value=0)\n        filtered_ds_test = data_set.filter(lambda x, y: tf.cast(table.lookup(y[0]), tf.bool)).map(lambda a, b: (a, b[1:])).batch(batch_size)\n        \n        if (len(input_size) == 2): #Need to hold all required data set sizes for all models\n            data_set_shaped_train = filtered_ds_train.map(lambda k, v: (tf.ensure_shape(k, [None, time_step, num_features]), tf.ensure_shape(v, [None, num_behaviors])), num_parallel_calls=tf.data.AUTOTUNE)\n            # Only flatten if the model asks for it flattened\n            if species:\n                data_set_train = data_set_shaped_train.map(lambda k, v: (tf.concat([tf.reshape(k[:, :, :num_features-1], [-1, (num_features-1) * time_step]), tf.reshape(k[:,0,num_features-1], (-1, 1))], 1), v), num_parallel_calls=tf.data.AUTOTUNE)\n            else:\n                data_set_train = data_set_shaped_train.map(lambda k, v: (tf.reshape(k, [-1, num_features * time_step]), v), num_parallel_calls=tf.data.AUTOTUNE)\n\n            data_set_shaped_test = filtered_ds_test.map(lambda k, v: (tf.ensure_shape(k, [None, time_step, num_features]), tf.ensure_shape(v, [None, num_behaviors])), num_parallel_calls=tf.data.AUTOTUNE)\n\n            if species:\n                data_set_test = data_set_shaped_test.map(lambda k, v: (tf.concat([tf.reshape(k[:, :, :num_features-1], [-1, (num_features-1) * time_step]), tf.reshape(k[:,0,num_features-1], (-1, 1))], 1), v), num_parallel_calls=tf.data.AUTOTUNE)\n            else:\n                data_set_test = data_set_shaped_test.map(lambda k, v: (tf.reshape(k, [-1, num_features * time_step]), v), num_parallel_calls=tf.data.AUTOTUNE)#.cache()\n        \n        else:\n            data_set_train = filtered_ds_train.map(lambda k, v: (tf.ensure_shape(k, [None, time_step, num_features]), tf.ensure_shape(v, [None, num_behaviors])), num_parallel_calls=tf.data.AUTOTUNE)\n            data_set_test = filtered_ds_test.map(lambda k, v: (tf.ensure_shape(k, [None, time_step, num_features]), tf.ensure_shape(v, [None, num_behaviors])),num_parallel_calls=tf.data.AUTOTUNE)\n\n        if cache == True:\n            data_set_train = data_set_train.cache()\n            data_set_train_shuffled = data_set_train\n            #data_set_test = data_set_test.cache() #Reduce memory usage by only caching the train set and minimizing test set usage.\n        else:\n            data_set_train_shuffled = data_set_train.shuffle(1000, reshuffle_each_iteration=True)\n        data_set_train = data_set_train.prefetch(tf.data.AUTOTUNE)\n        data_set_test = data_set_test.prefetch(tf.data.AUTOTUNE)\n        \n        for j in range(len(model_architecutres)):\n            if timed:\n                start = time.process_time()\n            \n            model = model_architecutres[j](num_features, time_step, len(behaviors))\n            model.fit(data_set_train_shuffled, epochs=epochs, validation_data = data_set_test, validation_freq = val_freq, verbose = 2)\n            model.save(f\"{model.name}_{species_name}_{i}_{animal_id}.keras\")\n            \n            if timed:\n                print(\"Time taken for Train: \")\n                print(time.process_time() - start)\n                start = time.process_time()\n            \n            if timed:\n                start = time.process_time()\n            model_preds_probs_train = model.predict(data_set_train, verbose = 2)\n            if timed:\n                print(\"Time taken to predict: \")\n                print(time.process_time() - start)\n                start = time.process_time()\n            \n            filter_train = np.nonzero(np.max(model_preds_probs_train, axis = 1) > threshold)\n            filtered_probs_train = np.take(model_preds_probs_train, filter_train[0], axis = 0)\n            if len(model_preds_probs_train) > len(filtered_probs_train):\n                print(f\"Removed {len(model_preds_probs_train)-len(filtered_probs_train)} estimates\")\n            \n            model_preds_train = np.argmax(filtered_probs_train, axis = 1)\n            mapped_preds_train = np.array([behaviors[x] for x in model_preds_train])\n            filtered_y_train = np.argmax(np.concatenate([y for _, y in data_set_train], axis=0), axis = 1).take(filter_train[0])\n            filtered_y_train = np.array([behaviors[y] for y in filtered_y_train])\n            \n            if timed:\n                start = time.process_time()\n            model_preds_probs_test = model.predict(data_set_test, verbose = 2)\n            if timed:\n                print(\"Time taken to predict: \")\n                print(time.process_time() - start)\n                start = time.process_time()\n\n            filter_test = np.nonzero(np.max(model_preds_probs_test, axis = 1) > threshold)\n            filtered_probs_test = np.take(model_preds_probs_test, filter_test[0], axis = 0)\n            if len(model_preds_probs_test) > len(filtered_probs_test):\n                print(f\"Removed {len(model_preds_probs_test)-len(filtered_probs_test)} estimates\")\n            \n            model_preds_test = np.argmax(filtered_probs_test, axis = 1)\n            mapped_preds_test = np.array([behaviors[x] for x in model_preds_test])\n            filtered_y_test = np.argmax(np.concatenate([y for _, y in data_set_test], axis=0), axis = 1).take(filter_test[0])\n            filtered_y_test = np.array([behaviors[y] for y in filtered_y_test])\n            \n            statistics[j][0] = statistics[j][0] + np.sum(mapped_preds_train == filtered_y_train) / num_splits\n            statistics[j][1] = statistics[j][1] + np.sum(mapped_preds_test == filtered_y_test) / num_splits\n            \n            compute_tss_keras(model, filtered_y_train, mapped_preds_train, Y.unique(), \"Training\", epochs, i, f\"{species_name}_{animal_id}\")\n            compute_tss_keras(model, filtered_y_test, mapped_preds_test, Y.unique(), \"Testing\", epochs, i, f\"{species_name}_{animal_id}\")\n\n            if transfer:\n                inner = keras.Model(inputs = model.layers[1].input, outputs = model.layers[-2].output)\n                inner.trainable = False\n                if species:\n                    inner.layers[3].trainable = True\n                inputs = keras.Input(shape=model.get_config()[\"layers\"][0][\"config\"][\"batch_shape\"][1:])\n                x = inner(inputs, training = False)\n                x = keras.layers.Dense(64, activation=\"relu\")(x)\n                x = keras.layers.Dropout(.2)(x)\n                outputs = keras.layers.Dense(len(behaviors), activation=\"softmax\")(x)\n                final = keras.Model(inputs=inputs, outputs=outputs, name = f\"{model.name}_transfer\")\n                final.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n\n                data_set_transfer = data_set_test.shard(10, 0).cache() #Take 10% of the test data to retrain (1/n %)\n\n                final.fit(data_set_transfer, epochs=int(epochs/2), validation_data = data_set_test, validation_freq = val_freq, verbose = 2)\n                final.save(f\"{final.name}_{species_name}_{i}.keras\")\n                model_preds_probs_train = final.predict(data_set_test, verbose = 2)\n                filter_train = np.nonzero(np.max(model_preds_probs_train, axis = 1) > threshold)\n                filtered_probs_train = np.take(model_preds_probs_train, filter_train[0], axis = 0)\n                if len(model_preds_probs_train) > len(filtered_probs_train):\n                    print(f\"Removed {len(model_preds_probs_train)-len(filtered_probs_train)} estimates\")\n                \n                model_preds_train = np.argmax(filtered_probs_train, axis = 1)\n                mapped_preds_train = np.array([behaviors[x] for x in model_preds_train])\n                filtered_y_train = np.argmax(np.concatenate([y for _, y in data_set_test], axis=0), axis = 1).take(filter_train[0])\n                filtered_y_train = np.array([behaviors[y] for y in filtered_y_train])\n                \n                compute_tss_keras(final, filtered_y_train, mapped_preds_train, Y.unique(), \"Transfer\", epochs, i, f\"{species_name}_{animal_id}\")\n                \n        i = i + 1\n    \n    return statistics","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.703530Z","iopub.execute_input":"2026-02-06T15:49:15.704321Z","iopub.status.idle":"2026-02-06T15:49:15.750632Z","shell.execute_reply.started":"2026-02-06T15:49:15.704288Z","shell.execute_reply":"2026-02-06T15:49:15.749044Z"},"papermill":{"duration":0.03485,"end_time":"2025-12-02T21:08:09.018113","exception":false,"start_time":"2025-12-02T21:08:08.983263","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":8},{"id":"630d7fd0","cell_type":"code","source":"def train_models(model_architecutres, data_set, Y, num_features, splits = None, num_splits = 5, time_step = 1, timed = False, threshold = 0, species = False, species_name = \"NA\", species_categories = []):\n    \"\"\" Statistics are reported in order of modesl w/ [train acc, test acc, train tss, test tss]\"\"\"\n    \n    behaviors = sorted(Y.unique())\n    num_behaviors = len(behaviors)\n\n    statistics = []\n    for model in model_architecutres:\n        statistics.append([0, 0, pd.DataFrame(columns = Y.unique()), pd.DataFrame(columns = Y.unique())])\n    \n    if (splits == None) or (splits == []): #Expecting a zipped list of indicies i.e. ((train indices 1, test indicies 1), (train indices 2, test indicies 2), etc.)\n        #If user does not provide splits we take num_splits to make stratified splits via skf\n        skf = model_selection.StratifiedKFold(n_splits=num_splits)\n        splits = []\n        for train, test in skf.split(np.zeros(len(Y.index)), Y):\n            splits.append((-1, train, test))\n        else:\n            num_splits = len(splits)\n            \n    i=0\n    for (animal_id, train_index, test_index) in splits:\n        if timed:\n            start = time.process_time()\n    \n        keys_tensor = tf.constant(train_index, dtype = np.int64)\n        vals_tensor = tf.ones_like(keys_tensor)  # Ones will be casted to True.\n        \n        table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n        default_value=0)  # If index not in table, return 0.\n        \n        #This massive things just looks up the index in the table\n        #If the index is present, return 1, else return 0. It casts those to bools, the maps the output data to remove the index info. Then we rebatch.\n        \n        filtered_ds_train = data_set.filter(lambda x, y: tf.cast(table.lookup(y[0]), tf.bool)).map(lambda a, b: (a, b[1:]))\n        \n        #We do this over for the test indicies (could use ~tf.cast but this allows for the possibility of not using all the data on a split)\n        keys_tensor = tf.constant(test_index, dtype = np.int64)\n        vals_tensor = tf.ones_like(keys_tensor)\n        \n        table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n        default_value=0)\n        filtered_ds_test = data_set.filter(lambda x, y: tf.cast(table.lookup(y[0]), tf.bool)).map(lambda a, b: (a, b[1:]))\n        \n        data_set_shaped_train = filtered_ds_train.map(lambda k, v: (tf.ensure_shape(k, [time_step, num_features]), tf.ensure_shape(v, [num_behaviors])))\n        # Only flatten if the model asks for it flattened\n        if species:\n            data_set_train = data_set_shaped_train.map(lambda k, v: (tf.concat([tf.reshape(k[:, :num_features-1], [-1, (num_features-1) * time_step]), tf.reshape(k[0,num_features-1], (-1, 1))], 1), v), num_parallel_calls=tf.data.AUTOTUNE)\n        else:\n            data_set_train = data_set_shaped_train.map(lambda k, v: (tf.reshape(k, [-1, num_features * time_step]), v))\n        \n        data_set_shaped_test = filtered_ds_test.map(lambda k, v: (tf.ensure_shape(k, [time_step, num_features]), tf.ensure_shape(v, [num_behaviors])))\n        # Only flatten if the model asks for it flattened\n        if species:\n            data_set_test = data_set_shaped_test.map(lambda k, v: (tf.concat([tf.reshape(k[:, :num_features-1], [-1, (num_features-1) * time_step]), tf.reshape(k[0,num_features-1], (-1, 1))], 1), v), num_parallel_calls=tf.data.AUTOTUNE)\n        else:\n            data_set_test = data_set_shaped_test.map(lambda k, v: (tf.reshape(k, [-1, num_features * time_step]), v))\n        \n        if timed:\n            print(\"Time taken for tensorflow processing: \")\n            print(time.process_time() - start)\n            start = time.process_time()\n        \n        #Scikit learn can't use tensorflow datasets\n        if species:\n            train_x = []\n            train_y = []\n            train_species = []\n            for x, y in data_set_train:\n                train_x.append(x[:, :(time_step) * (num_features-1)])\n                train_y.append(behaviors[np.argmax(y.numpy())])\n                train_species.append(x[0, (time_step) * (num_features-1)].numpy())\n\n            train_x = np.concatenate(train_x, axis = 0)\n            train_x = np.concatenate([train_x, pd.get_dummies(pd.Categorical(train_species, categories = species_categories)).to_numpy()], axis = 1)\n            train_y = np.array(train_y)\n        else:\n            train_x = []\n            train_y = []\n            for x, y in data_set_train:\n                train_x.append(x)\n                train_y.append(behaviors[np.argmax(y.numpy())])\n    \n            train_x = np.concatenate(train_x, axis = 0)\n            train_y = np.array(train_y)\n            \n        if species:\n            test_x = []\n            test_y = []\n            test_species = []\n            for x, y in data_set_test:\n                test_x.append(x[:, :(time_step) * (num_features-1)])\n                test_y.append(behaviors[np.argmax(y.numpy())])\n                test_species.append(x[0, (time_step) * (num_features-1)].numpy())\n                \n            test_x = np.concatenate(test_x, axis = 0)\n            test_x = np.concatenate([test_x, pd.get_dummies(pd.Categorical(test_species, categories = species_categories)).to_numpy()], axis = 1)\n            test_y = np.array(test_y)\n\n        else:\n            test_x = []\n            test_y = []\n            for x, y in data_set_test:\n                test_x.append(x)\n                test_y.append(behaviors[np.argmax(y.numpy())])\n            \n            test_x = np.concatenate(test_x, axis = 0)\n            test_y = np.array(test_y)\n        \n        if timed:\n            print(\"Time taken for np concatenation: \")\n            print(time.process_time() - start)\n            start = time.process_time()\n        print(\"Finished data processing\")\n        for j in range(len(model_architecutres)):\n        \n            model = model_architecutres[j]\n            model.fit(train_x, train_y)\n            with open(f\"{type(model).__name__}_{species_name}_{i}.pkl\", \"wb\") as f:\n                dump(model, f, protocol=5)\n            if timed:\n                print(\"Time taken to fit: \")\n                print(time.process_time() - start)\n                start = time.process_time()\n            #Produce the model predictions once for test & train\n            \n            if timed:\n                start = time.process_time()\n            model_preds_probs_train = model.predict_proba(train_x)\n            if timed:\n                print(\"Time taken to predict: \")\n                print(time.process_time() - start)\n                start = time.process_time()\n            \n            filter_train = np.nonzero(np.max(model_preds_probs_train, axis = 1) > threshold)\n            filtered_probs_train = model_preds_probs_train[filter_train]\n            if len(model_preds_probs_train) > len(filtered_probs_train):\n                print(f\"Removed {len(model_preds_probs_train)-len(filtered_probs_train)} estimates\")\n            \n            model_preds_train = np.argmax(filtered_probs_train, axis = 1)\n            classes = model.classes_\n            mapped_preds_train = np.array([classes[x] for x in model_preds_train])\n            filtered_y_train = train_y.take(filter_train[0])\n            \n            if timed:\n                start = time.process_time()\n            model_preds_probs_test = model.predict_proba(test_x)\n            if timed:\n                print(\"Time taken to predict: \")\n                print(time.process_time() - start)\n                start = time.process_time()\n            \n            filter_test = np.nonzero(np.max(model_preds_probs_test, axis = 1) > threshold)\n            filtered_probs_test = model_preds_probs_test[filter_test]\n            if len(model_preds_probs_test) > len(filtered_probs_test):\n                print(f\"Removed {len(model_preds_probs_test)-len(filtered_probs_test)} estimates\")\n            \n            model_preds_test = np.argmax(filtered_probs_test, axis = 1)\n            mapped_preds_test = np.array([classes[x] for x in model_preds_test])\n            filtered_y_test = test_y.take(filter_test[0])\n            \n            statistics[j][0] = statistics[j][0] + np.sum(mapped_preds_train == filtered_y_train) / num_splits\n            statistics[j][1] = statistics[j][1] + np.sum(mapped_preds_test == filtered_y_test) / num_splits\n            if timed:\n                print(\"Time taken to score: \")\n                print(time.process_time() - start)\n            \n            compute_tss(model, filtered_y_train, mapped_preds_train, Y.unique(), \"Training\", i, f\"{species_name}_{animal_id}\")\n            compute_tss(model, filtered_y_test, mapped_preds_test, Y.unique(), \"Testing\", i, f\"{species_name}_{animal_id}\")\n        i = i+1\n    return statistics","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.752513Z","iopub.execute_input":"2026-02-06T15:49:15.752859Z","iopub.status.idle":"2026-02-06T15:49:15.788784Z","shell.execute_reply.started":"2026-02-06T15:49:15.752834Z","shell.execute_reply":"2026-02-06T15:49:15.787544Z"},"papermill":{"duration":0.026918,"end_time":"2025-12-02T21:08:09.050189","exception":false,"start_time":"2025-12-02T21:08:09.023271","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":9},{"id":"4a276729","cell_type":"markdown","source":"Keras model specifications","metadata":{}},{"id":"e3393df6","cell_type":"code","source":"def test_model(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=(input_cols * time_step,))\n    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(128, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(128, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs=inputs, outputs=outputs, name = \"BaseNN\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.789629Z","iopub.execute_input":"2026-02-06T15:49:15.789919Z","iopub.status.idle":"2026-02-06T15:49:15.825952Z","shell.execute_reply.started":"2026-02-06T15:49:15.789893Z","shell.execute_reply":"2026-02-06T15:49:15.824739Z"},"papermill":{"duration":0.012066,"end_time":"2025-12-02T21:08:09.083941","exception":false,"start_time":"2025-12-02T21:08:09.071875","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":10},{"id":"9c01c53d","cell_type":"code","source":"def test_model_species(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=((input_cols-1) * time_step + 1,))\n    species = keras.layers.Lambda(lambda x: x[:, (input_cols-1) * time_step])(inputs)\n    data = keras.layers.Lambda(lambda x: x[:, :(input_cols-1) * time_step])(inputs)\n    embed = keras.layers.Embedding(13, 32)(species)\n    y = keras.layers.Concatenate()([data, embed])\n    x = keras.layers.Dense(256, activation=\"relu\")(y)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(128, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(128, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs=inputs, outputs=outputs, name = \"BaseNNSpecies\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.826913Z","iopub.execute_input":"2026-02-06T15:49:15.827188Z","iopub.status.idle":"2026-02-06T15:49:15.862069Z","shell.execute_reply.started":"2026-02-06T15:49:15.827165Z","shell.execute_reply":"2026-02-06T15:49:15.860741Z"},"papermill":{"duration":0.012744,"end_time":"2025-12-02T21:08:09.102016","exception":false,"start_time":"2025-12-02T21:08:09.089272","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":11},{"id":"2d70bc5d","cell_type":"code","source":"def lstm_model(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=(time_step, input_cols))\n    x = keras.layers.LSTM(64, recurrent_dropout = .1, dropout = .1)(inputs)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs=inputs, outputs=outputs, name = \"LSTM\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.863219Z","iopub.execute_input":"2026-02-06T15:49:15.863505Z","iopub.status.idle":"2026-02-06T15:49:15.894383Z","shell.execute_reply.started":"2026-02-06T15:49:15.863464Z","shell.execute_reply":"2026-02-06T15:49:15.892771Z"},"papermill":{"duration":0.010722,"end_time":"2025-12-02T21:08:09.117775","exception":false,"start_time":"2025-12-02T21:08:09.107053","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":12},{"id":"ef677ee8","cell_type":"code","source":"def lstm_model_species(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=(time_step, input_cols))\n    species = keras.layers.Lambda(lambda x: x[:, 0, input_cols-1])(inputs)\n    data = keras.layers.Lambda(lambda x: x[:, :, :input_cols-1])(inputs)\n    embed = keras.layers.Embedding(13, 32)(species)\n    x = keras.layers.LSTM(64, recurrent_dropout = .1, dropout = .1)(data) #Only use the recurrent connection on the actual time data\n    y = keras.layers.Concatenate()([x, embed])\n    x = keras.layers.Dense(64, activation=\"relu\")(y)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs=inputs, outputs=outputs, name = \"LSTMSpecies\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.895600Z","iopub.execute_input":"2026-02-06T15:49:15.895976Z","iopub.status.idle":"2026-02-06T15:49:15.926862Z","shell.execute_reply.started":"2026-02-06T15:49:15.895933Z","shell.execute_reply":"2026-02-06T15:49:15.925567Z"},"papermill":{"duration":0.01148,"end_time":"2025-12-02T21:08:09.134254","exception":false,"start_time":"2025-12-02T21:08:09.122774","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":13},{"id":"4cd270cd","cell_type":"code","source":"def gru_model(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=(time_step, input_cols))\n    x = keras.layers.GRU(64, recurrent_dropout = .1, dropout = .1)(inputs)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs=inputs, outputs=outputs, name = \"GRU\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.928085Z","iopub.execute_input":"2026-02-06T15:49:15.928634Z","iopub.status.idle":"2026-02-06T15:49:15.963585Z","shell.execute_reply.started":"2026-02-06T15:49:15.928602Z","shell.execute_reply":"2026-02-06T15:49:15.962448Z"},"papermill":{"duration":0.010658,"end_time":"2025-12-02T21:08:09.149869","exception":false,"start_time":"2025-12-02T21:08:09.139211","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":14},{"id":"56d1e12f","cell_type":"code","source":"def gru_model_species(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=(time_step, input_cols))\n    species = keras.layers.Lambda(lambda x: x[:, 0, input_cols-1])(inputs)\n    data = keras.layers.Lambda(lambda x: x[:, :, :input_cols-1])(inputs)\n    embed = keras.layers.Embedding(13, 32)(species)\n    x = keras.layers.GRU(64, recurrent_dropout = .1, dropout = .1)(data)\n    y = keras.layers.Concatenate()([x, embed])\n    x = keras.layers.Dense(64, activation=\"relu\")(y)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs=inputs, outputs=outputs, name = \"GRUSpecies\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.964587Z","iopub.execute_input":"2026-02-06T15:49:15.964933Z","iopub.status.idle":"2026-02-06T15:49:15.986311Z","shell.execute_reply.started":"2026-02-06T15:49:15.964903Z","shell.execute_reply":"2026-02-06T15:49:15.985333Z"},"papermill":{"duration":0.011885,"end_time":"2025-12-02T21:08:09.166853","exception":false,"start_time":"2025-12-02T21:08:09.154968","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":15},{"id":"65c9bd28","cell_type":"code","source":"def cnn_model(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=(time_step, input_cols))\n    x = keras.layers.Conv1D(filters=32, kernel_size=10, padding=\"same\")(inputs)\n\n    x = keras.layers.Conv1D(filters=32, kernel_size=5, padding=\"same\")(x)\n    x = keras.layers.GlobalMaxPooling1D()(x)\n    \n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs=inputs, outputs=outputs, name = \"CNN\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:15.987834Z","iopub.execute_input":"2026-02-06T15:49:15.988296Z","iopub.status.idle":"2026-02-06T15:49:16.018660Z","shell.execute_reply.started":"2026-02-06T15:49:15.988243Z","shell.execute_reply":"2026-02-06T15:49:16.017561Z"},"papermill":{"duration":0.011691,"end_time":"2025-12-02T21:08:09.183644","exception":false,"start_time":"2025-12-02T21:08:09.171953","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":16},{"id":"e0efa10e","cell_type":"code","source":"def cnn_model_species(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=(time_step, input_cols))\n    species = keras.layers.Lambda(lambda x: x[:, 0, input_cols-1])(inputs)\n    data = keras.layers.Lambda(lambda x: x[:, :, :input_cols-1])(inputs)\n    embed = keras.layers.Embedding(13, 32)(species)\n    \n    x = keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\")(data)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.ReLU()(x)\n    \n    x = keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\")(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.ReLU()(x)\n    \n    #May also want to try max pooling and pooling along each step\n    x = keras.layers.GlobalAveragePooling1D()(x)\n    y = keras.layers.Concatenate()([x, embed])\n    \n    x = keras.layers.Dense(64, activation=\"relu\")(y)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs=inputs, outputs=outputs, name = \"CNNSpecies\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:16.020584Z","iopub.execute_input":"2026-02-06T15:49:16.020997Z","iopub.status.idle":"2026-02-06T15:49:16.046928Z","shell.execute_reply.started":"2026-02-06T15:49:16.020966Z","shell.execute_reply":"2026-02-06T15:49:16.045579Z"},"papermill":{"duration":0.012382,"end_time":"2025-12-02T21:08:09.201053","exception":false,"start_time":"2025-12-02T21:08:09.188671","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":17},{"id":"dcadae32","cell_type":"code","source":"def transformer_model(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=(time_step, input_cols))\n    \n    pos_embed = keras_hub.layers.PositionEmbedding(time_step)(inputs)\n    inputs_embed = inputs + pos_embed\n    \n    # Attention and Normalization\n    x = keras.layers.MultiHeadAttention(\n      key_dim=64, num_heads=8, dropout=.2\n    )(inputs_embed, inputs_embed)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n    res = x + inputs_embed\n    \n    # Feed Forward Part\n    x = keras.layers.Conv1D(filters=32, kernel_size=1, activation=\"relu\")(res)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Conv1D(filters=inputs_embed.shape[-1], kernel_size=1)(x)\n    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n    x_res = x + res\n\n    #x = keras.layers.MultiHeadAttention(\n    #  key_dim=64, num_heads=8, dropout=.2)(x_res, x_res)\n    #x = keras.layers.Dropout(.2)(x)\n    #x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n    #res = x + x_res\n    \n    # Feed Forward Part\n    #x = keras.layers.Conv1D(filters=32, kernel_size=1, activation=\"relu\")(res)\n    #x = keras.layers.Dropout(.2)(x)\n    #x = keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n    #x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n    #x = x + res\n    \n    x = keras.layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs, outputs, name = \"Transformer\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:16.048097Z","iopub.execute_input":"2026-02-06T15:49:16.048595Z","iopub.status.idle":"2026-02-06T15:49:16.078111Z","shell.execute_reply.started":"2026-02-06T15:49:16.048563Z","shell.execute_reply":"2026-02-06T15:49:16.076946Z"},"papermill":{"duration":0.012884,"end_time":"2025-12-02T21:08:09.218976","exception":false,"start_time":"2025-12-02T21:08:09.206092","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":18},{"id":"9009fb1f","cell_type":"code","source":"def transformer_model_species(input_cols = 69, time_step = 4, num_behaviors = 14):\n    inputs = keras.Input(shape=(time_step, input_cols))\n    \n    species = keras.layers.Lambda(lambda x: x[:, :, input_cols-1])(inputs)\n    data = keras.layers.Lambda(lambda x: x[:, :, :input_cols-1])(inputs)\n    species_embed = keras.layers.Embedding(13, 32)(species)\n    \n    \n    pos_embed = keras_hub.layers.PositionEmbedding(time_step)(data)\n    data_embed = data + pos_embed\n    y = keras.layers.Concatenate()([data_embed, species_embed])\n    \n    # Attention and Normalization\n    x = keras.layers.MultiHeadAttention(\n      key_dim=64, num_heads=8, dropout=.2\n    )(y, y)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n    res = x + y\n    \n    # Feed Forward Part\n    x = keras.layers.Conv1D(filters=64, kernel_size=1, activation=\"relu\")(res)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Conv1D(filters=y.shape[-1], kernel_size=1)(x)\n    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n    x = x + res\n    \n    x = keras.layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(.2)(x)\n    outputs = keras.layers.Dense(num_behaviors, activation=\"softmax\")(x)\n    model = keras.Model(inputs, outputs, name = \"TransformerSpecies\")\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:16.079220Z","iopub.execute_input":"2026-02-06T15:49:16.079644Z","iopub.status.idle":"2026-02-06T15:49:16.106294Z","shell.execute_reply.started":"2026-02-06T15:49:16.079609Z","shell.execute_reply":"2026-02-06T15:49:16.104649Z"},"papermill":{"duration":0.01342,"end_time":"2025-12-02T21:08:09.237326","exception":false,"start_time":"2025-12-02T21:08:09.223906","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":19},{"id":"5161f65c","cell_type":"markdown","source":"Dataset creation and splitting","metadata":{}},{"id":"b554d039","cell_type":"code","source":"def make_target_group_splits(ids, target_ids = None): \n    #Ids are the groups to split on\n    #Target id is the group that will remain in the testing set\n    train_index = []\n    test_index = []\n    if target_ids == None:\n        target_ids = ids.unique()\n    for animal in target_ids:\n        train_index.append(ids[ids != animal].index.tolist())\n        test_index.append(ids[ids == animal].index.tolist())\n      \n    return list(zip(target_ids, train_index, test_index))","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:16.107803Z","iopub.execute_input":"2026-02-06T15:49:16.108298Z","iopub.status.idle":"2026-02-06T15:49:16.148149Z","shell.execute_reply.started":"2026-02-06T15:49:16.108262Z","shell.execute_reply":"2026-02-06T15:49:16.147087Z"},"papermill":{"duration":0.010394,"end_time":"2025-12-02T21:08:09.268093","exception":false,"start_time":"2025-12-02T21:08:09.257699","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":20},{"id":"3d53a613","cell_type":"code","source":"def make_splits(ids, num_splits = 5, frac_train = None):\n    #Default behavior is StratifiedKFold \n    #Only usable with frac_train <= .5\n    splits = []\n    if frac_train == None:\n        skf = model_selection.StratifiedKFold(n_splits = num_splits)\n        for train, test in skf.split(np.zeros(len(ids.index)), ids):\n            splits.append((-1, train, test))\n        return splits\n    if(frac_train > .5):\n        print(\"make_splits only works with frac_train < .5\")\n        return\n    skf = model_selection.StratifiedKFold(n_splits=int(1/frac_train))\n    num_splits = min(int(1/frac_train), num_splits)\n    indices = skf.split(np.zeros(len(ids.index)), ids)\n    for i in range(num_splits):\n        train, test = indices.__next__()\n        splits.append((-1, test, train))\n    return splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T15:49:16.156327Z","iopub.execute_input":"2026-02-06T15:49:16.157998Z","iopub.status.idle":"2026-02-06T15:49:16.204690Z","shell.execute_reply.started":"2026-02-06T15:49:16.157958Z","shell.execute_reply":"2026-02-06T15:49:16.201585Z"}},"outputs":[],"execution_count":21},{"id":"ef88a1fc","cell_type":"code","source":"def make_dataset(X, Y, time_step, ids):\n    dummified = pd.get_dummies(Y)\n    dummified = dummified.reindex(sorted(dummified.columns), axis=1)\n    tempX = X[ids == ids.unique()[0]]\n    tempDummy = dummified[ids == ids.unique()[0]]\n    data_set = tf.keras.preprocessing.timeseries_dataset_from_array(np.array(tempX), np.array(tempDummy.reset_index(), dtype = int), sequence_length= time_step, sequence_stride=1, batch_size = None)\n    for id in segment_ids.unique()[1:]:\n        tempX = X[ids == id]\n        tempDummy = dummified[ids == id]\n        data_set = data_set.concatenate(tf.keras.preprocessing.timeseries_dataset_from_array(np.array(tempX), np.array(tempDummy.reset_index(), dtype = int), sequence_length= time_step, sequence_stride=1, batch_size = None))\n    del tempX\n    del tempDummy\n    del dummified\n    gc.collect()\n    return data_set","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:16.206684Z","iopub.execute_input":"2026-02-06T15:49:16.207406Z","iopub.status.idle":"2026-02-06T15:49:16.255039Z","shell.execute_reply.started":"2026-02-06T15:49:16.207194Z","shell.execute_reply":"2026-02-06T15:49:16.253005Z"},"papermill":{"duration":0.01156,"end_time":"2025-12-02T21:08:09.284697","exception":false,"start_time":"2025-12-02T21:08:09.273137","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":22},{"id":"b1b248c0","cell_type":"code","source":"data_set = make_dataset(X, Y, sample_rate * time_window, segment_ids)","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:16.256280Z","iopub.execute_input":"2026-02-06T15:49:16.256603Z","iopub.status.idle":"2026-02-06T15:49:23.996876Z","shell.execute_reply.started":"2026-02-06T15:49:16.256573Z","shell.execute_reply":"2026-02-06T15:49:23.995718Z"},"papermill":{"duration":5.779735,"end_time":"2025-12-02T21:08:15.069474","exception":false,"start_time":"2025-12-02T21:08:09.289739","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"2026-02-06 15:49:16.335430: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":23},{"id":"5cff8163","cell_type":"code","source":"num_splits = 4 #Overriden if splits are provided directly. Using this will stratify based on Y passed to training code\nfrac_test = .1\nsplits = [] #Zipped list of split id, train, test indicies. Split id is only used for reference purposes and can be set to any value\n\n#splits = make_splits(animal_ids, 4)\n\n#for animal in animal_ids.unique(): #Used to produce a 1/10 training amount\n#    animal_index = animal_ids[animal_ids == animal].index.tolist()\n#    temp = Y.iloc[animal_index]\n#    splits_temp = make_splits(temp, num_splits, frac_test)\n#    splits.append((animal, temp.iloc[splits_temp[0][1]].index.to_numpy(), temp.iloc[splits_temp[0][2]].index.to_numpy()))\n\nsplits = make_splits(Y + animal_ids.apply(str), 2, .1)\n\n#splits = make_target_group_splits(animal_ids)\n#splits = make_target_group_splits(animal_ids, [4])","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:23.998002Z","iopub.execute_input":"2026-02-06T15:49:23.998319Z","iopub.status.idle":"2026-02-06T15:49:24.783640Z","shell.execute_reply.started":"2026-02-06T15:49:23.998283Z","shell.execute_reply":"2026-02-06T15:49:24.782681Z"},"papermill":{"duration":0.413508,"end_time":"2025-12-02T21:08:15.488588","exception":false,"start_time":"2025-12-02T21:08:15.075080","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":24},{"id":"555ae7c0","cell_type":"code","source":"svm_model = svm.LinearSVC()\nclf_model = calibration.CalibratedClassifierCV(svm_model)\nknn_model = neighbors.KNeighborsClassifier(n_neighbors = 100, n_jobs = -1)\nrf_model = ensemble.RandomForestClassifier(1000, n_jobs = -1, min_samples_leaf = 100)\nlog_model = linear_model.LogisticRegression(solver = \"saga\")\n#stats_animal_based_splits = train_models([log_model], data_set, Y, len(X.columns), splits = splits, num_splits = num_splits, time_step = sample_rate * time_window, timed = False, species_name = sample_name, species = use_species, species_categories = animal_ids.unique())","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:24.784515Z","iopub.execute_input":"2026-02-06T15:49:24.784836Z","iopub.status.idle":"2026-02-06T15:49:24.790610Z","shell.execute_reply.started":"2026-02-06T15:49:24.784809Z","shell.execute_reply":"2026-02-06T15:49:24.789434Z"},"papermill":{"duration":392.903207,"end_time":"2025-12-02T21:14:48.429353","exception":false,"start_time":"2025-12-02T21:08:15.526146","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":25},{"id":"1dbc14dd","cell_type":"code","source":"#Note that models that have different input size (2-D vs 3-D) cannot be trained together\ntrain_models_keras([test_model_species], data_set, Y, len(X.columns), splits = splits, num_splits = num_splits, epochs = 30, time_step = sample_rate * time_window, timed = False, batch_size = 256, species = use_species, species_name = sample_name, cache = True, val_freq = 10000, transfer = transfer)\n#train_models_keras([gru_model, transformer_model], data_set, Y, len(X.columns), splits = splits, num_splits = num_splits, epochs = 100, time_step = sample_rate * time_window, timed = False, batch_size = 256, species = use_species, species_name = sample_name, cache = True, val_freq = 10000000, transfer = transfer)","metadata":{"execution":{"iopub.status.busy":"2026-02-06T15:49:24.791861Z","iopub.execute_input":"2026-02-06T15:49:24.792146Z","iopub.status.idle":"2026-02-06T16:05:46.496630Z","shell.execute_reply.started":"2026-02-06T15:49:24.792118Z","shell.execute_reply":"2026-02-06T16:05:46.495431Z"},"papermill":{"duration":1060.956361,"end_time":"2025-12-02T21:32:29.391405","exception":false,"start_time":"2025-12-02T21:14:48.435044","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/30\n163/163 - 72s - 439ms/step - categorical_accuracy: 0.6451 - loss: 0.8924\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"name":"stdout","text":"163/163 - 2s - 14ms/step - categorical_accuracy: 0.7624 - loss: 0.6276\nEpoch 3/30\n163/163 - 2s - 12ms/step - categorical_accuracy: 0.7883 - loss: 0.5501\nEpoch 4/30\n163/163 - 2s - 12ms/step - categorical_accuracy: 0.8009 - loss: 0.5057\nEpoch 5/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.8145 - loss: 0.4675\nEpoch 6/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.8275 - loss: 0.4293\nEpoch 7/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.8372 - loss: 0.4055\nEpoch 8/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.8471 - loss: 0.3755\nEpoch 9/30\n163/163 - 2s - 10ms/step - categorical_accuracy: 0.8589 - loss: 0.3522\nEpoch 10/30\n163/163 - 2s - 10ms/step - categorical_accuracy: 0.8688 - loss: 0.3307\nEpoch 11/30\n163/163 - 2s - 10ms/step - categorical_accuracy: 0.8750 - loss: 0.3148\nEpoch 12/30\n163/163 - 2s - 10ms/step - categorical_accuracy: 0.8840 - loss: 0.2905\nEpoch 13/30\n163/163 - 2s - 10ms/step - categorical_accuracy: 0.8897 - loss: 0.2801\nEpoch 14/30\n163/163 - 2s - 10ms/step - categorical_accuracy: 0.8984 - loss: 0.2615\nEpoch 15/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.9006 - loss: 0.2531\nEpoch 16/30\n163/163 - 2s - 13ms/step - categorical_accuracy: 0.9083 - loss: 0.2380\nEpoch 17/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.9107 - loss: 0.2316\nEpoch 18/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.9165 - loss: 0.2164\nEpoch 19/30\n163/163 - 2s - 10ms/step - categorical_accuracy: 0.9212 - loss: 0.2100\nEpoch 20/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.9245 - loss: 0.1996\nEpoch 21/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.9262 - loss: 0.1947\nEpoch 22/30\n163/163 - 2s - 11ms/step - categorical_accuracy: 0.9277 - loss: 0.1883\nEpoch 23/30\n163/163 - 2s - 13ms/step - categorical_accuracy: 0.9346 - loss: 0.1739\nEpoch 24/30\n163/163 - 2s - 12ms/step - categorical_accuracy: 0.9354 - loss: 0.1709\nEpoch 25/30\n163/163 - 2s - 12ms/step - categorical_accuracy: 0.9387 - loss: 0.1649\nEpoch 26/30\n163/163 - 2s - 12ms/step - categorical_accuracy: 0.9407 - loss: 0.1605\nEpoch 27/30\n163/163 - 2s - 12ms/step - categorical_accuracy: 0.9416 - loss: 0.1560\nEpoch 28/30\n163/163 - 2s - 13ms/step - categorical_accuracy: 0.9438 - loss: 0.1496\nEpoch 29/30\n163/163 - 2s - 10ms/step - categorical_accuracy: 0.9447 - loss: 0.1450\nEpoch 30/30\n163/163 - 2s - 10ms/step - categorical_accuracy: 0.9486 - loss: 0.1395\n163/163 - 2s - 12ms/step\n1456/1456 - 75s - 52ms/step\nEpoch 1/15\n146/146 - 75s - 516ms/step - categorical_accuracy: 0.5306 - loss: 1.5225\nEpoch 2/15\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"name":"stdout","text":"146/146 - 2s - 11ms/step - categorical_accuracy: 0.6097 - loss: 1.1435\nEpoch 3/15\n146/146 - 2s - 11ms/step - categorical_accuracy: 0.6144 - loss: 1.0980\nEpoch 4/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6237 - loss: 1.0610\nEpoch 5/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6238 - loss: 1.0399\nEpoch 6/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6284 - loss: 1.0323\nEpoch 7/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6318 - loss: 1.0191\nEpoch 8/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6329 - loss: 1.0089\nEpoch 9/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6363 - loss: 1.0019\nEpoch 10/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6347 - loss: 0.9979\nEpoch 11/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6392 - loss: 0.9944\nEpoch 12/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6417 - loss: 0.9866\nEpoch 13/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6419 - loss: 0.9840\nEpoch 14/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6440 - loss: 0.9763\nEpoch 15/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6447 - loss: 0.9723\n1456/1456 - 68s - 47ms/step\nEpoch 1/30\n162/162 - 66s - 409ms/step - categorical_accuracy: 0.6348 - loss: 0.9528\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"name":"stdout","text":"162/162 - 2s - 10ms/step - categorical_accuracy: 0.7356 - loss: 0.6872\nEpoch 3/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.7658 - loss: 0.5957\nEpoch 4/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.7851 - loss: 0.5453\nEpoch 5/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.8025 - loss: 0.5020\nEpoch 6/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.8133 - loss: 0.4684\nEpoch 7/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.8249 - loss: 0.4354\nEpoch 8/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.8394 - loss: 0.4036\nEpoch 9/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.8508 - loss: 0.3757\nEpoch 10/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.8602 - loss: 0.3558\nEpoch 11/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.8687 - loss: 0.3314\nEpoch 12/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.8743 - loss: 0.3160\nEpoch 13/30\n162/162 - 2s - 11ms/step - categorical_accuracy: 0.8852 - loss: 0.2947\nEpoch 14/30\n162/162 - 2s - 11ms/step - categorical_accuracy: 0.8902 - loss: 0.2819\nEpoch 15/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.8940 - loss: 0.2710\nEpoch 16/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9021 - loss: 0.2533\nEpoch 17/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9068 - loss: 0.2421\nEpoch 18/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9097 - loss: 0.2349\nEpoch 19/30\n162/162 - 2s - 11ms/step - categorical_accuracy: 0.9140 - loss: 0.2224\nEpoch 20/30\n162/162 - 2s - 11ms/step - categorical_accuracy: 0.9189 - loss: 0.2106\nEpoch 21/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9220 - loss: 0.2067\nEpoch 22/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9243 - loss: 0.1975\nEpoch 23/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9279 - loss: 0.1876\nEpoch 24/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9339 - loss: 0.1739\nEpoch 25/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9316 - loss: 0.1817\nEpoch 26/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9345 - loss: 0.1739\nEpoch 27/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9368 - loss: 0.1670\nEpoch 28/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9401 - loss: 0.1632\nEpoch 29/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9388 - loss: 0.1610\nEpoch 30/30\n162/162 - 2s - 10ms/step - categorical_accuracy: 0.9435 - loss: 0.1516\n162/162 - 2s - 11ms/step\n1457/1457 - 70s - 48ms/step\nEpoch 1/15\n146/146 - 72s - 492ms/step - categorical_accuracy: 0.5939 - loss: 1.2962\nEpoch 2/15\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"name":"stdout","text":"146/146 - 1s - 9ms/step - categorical_accuracy: 0.6234 - loss: 1.1382\nEpoch 3/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6272 - loss: 1.0880\nEpoch 4/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6266 - loss: 1.0612\nEpoch 5/15\n146/146 - 1s - 10ms/step - categorical_accuracy: 0.6281 - loss: 1.0441\nEpoch 6/15\n146/146 - 1s - 10ms/step - categorical_accuracy: 0.6322 - loss: 1.0197\nEpoch 7/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6309 - loss: 1.0181\nEpoch 8/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6330 - loss: 1.0034\nEpoch 9/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6381 - loss: 0.9924\nEpoch 10/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6420 - loss: 0.9842\nEpoch 11/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6437 - loss: 0.9783\nEpoch 12/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6426 - loss: 0.9765\nEpoch 13/15\n146/146 - 1s - 10ms/step - categorical_accuracy: 0.6451 - loss: 0.9677\nEpoch 14/15\n146/146 - 1s - 9ms/step - categorical_accuracy: 0.6438 - loss: 0.9674\nEpoch 15/15\n146/146 - 1s - 10ms/step - categorical_accuracy: 0.6444 - loss: 0.9636\n1457/1457 - 64s - 44ms/step\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[[40886.0,\n  227455.0,\n  Empty DataFrame\n  Columns: [Stationary, Walking, Swimming, Grooming, Eating, Shaking, Social]\n  Index: [],\n  Empty DataFrame\n  Columns: [Stationary, Walking, Swimming, Grooming, Eating, Shaking, Social]\n  Index: []]]"},"metadata":{}}],"execution_count":26}]}